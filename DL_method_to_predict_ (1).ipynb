{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "Z2_-HuwOpNEJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7252cab5"
      },
      "source": [
        "##Abstarct\n",
        "This study presents a deep learning approach using PyTorch to predict breast cancer diagnoses from clinical and imaging-derived features. A publicly available breast cancer dataset was preprocessed by handling missing values, encoding categorical variables, and scaling numerical features. An artificial neural network (ANN) with two hidden layers was designed, employing ReLU activations and a sigmoid output for binary classification. The model was trained for 100 epochs using binary cross-entropy loss and the Adam optimizer. Evaluation on the held-out test set achieved an accuracy of 96.5%, demonstrating the modelâ€™s strong predictive capability. These results indicate that PyTorch-based deep learning models can effectively classify malignant and benign breast cancer cases. The framework can be further extended with hyperparameter optimization, alternative architectures, or cross-validation for enhanced robustness, and it highlights the potential of deep learning to support computer-aided diagnostic systems in oncology."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "PbGfeSgXwpw-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/Breast_cancer_dataset.csv')"
      ],
      "metadata": {
        "id": "FF__6L80wv_-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3278f494"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Handle missing values if any, encode categorical variables (the 'diagnosis' column), and scale the features. Split the data into training and testing sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a358afe"
      },
      "source": [
        "**Reasoning**:\n",
        "Inspect the DataFrame for missing values and handle the 'Unnamed: 32' column which has only missing values. Encode the 'diagnosis' column and split the data into features and target.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f769f99a",
        "outputId": "34de3d51-61a9-41f6-f538-9fb82ad153aa"
      },
      "source": [
        "print(df.isnull().sum())\n",
        "\n",
        "# The column 'Unnamed: 32' has only missing values, so we drop it.\n",
        "df = df.drop('Unnamed: 32', axis=1)\n",
        "\n",
        "# Encode the 'diagnosis' column (M=1, B=0)\n",
        "df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
        "\n",
        "# Separate features (X) and target (y), dropping 'id'\n",
        "X = df.drop(['id', 'diagnosis'], axis=1)\n",
        "y = df['diagnosis']"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id                           0\n",
            "diagnosis                    0\n",
            "radius_mean                  0\n",
            "texture_mean                 0\n",
            "perimeter_mean               0\n",
            "area_mean                    0\n",
            "smoothness_mean              0\n",
            "compactness_mean             0\n",
            "concavity_mean               0\n",
            "concave points_mean          0\n",
            "symmetry_mean                0\n",
            "fractal_dimension_mean       0\n",
            "radius_se                    0\n",
            "texture_se                   0\n",
            "perimeter_se                 0\n",
            "area_se                      0\n",
            "smoothness_se                0\n",
            "compactness_se               0\n",
            "concavity_se                 0\n",
            "concave points_se            0\n",
            "symmetry_se                  0\n",
            "fractal_dimension_se         0\n",
            "radius_worst                 0\n",
            "texture_worst                0\n",
            "perimeter_worst              0\n",
            "area_worst                   0\n",
            "smoothness_worst             0\n",
            "compactness_worst            0\n",
            "concavity_worst              0\n",
            "concave points_worst         0\n",
            "symmetry_worst               0\n",
            "fractal_dimension_worst      0\n",
            "Unnamed: 32                569\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "071f7a08"
      },
      "source": [
        "**Reasoning**:\n",
        "Scale the features and split the data into training and testing sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0de8786"
      },
      "source": [
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3b0694a"
      },
      "source": [
        "## Model definition\n",
        "\n",
        "### Subtask:\n",
        "Define a simple deep learning model using PyTorch. This could be a feedforward neural network for this dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a88bd06c"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a PyTorch neural network class for binary classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce4f0a98",
        "outputId": "895c6736-b10b-4bcd-f2a7-d0cd9e073ed0"
      },
      "source": [
        "class BreastCancerNet(nn.Module):\n",
        "    def __init__(self, input_features):\n",
        "        super(BreastCancerNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_features, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "input_features = X_train.shape[1]\n",
        "model = BreastCancerNet(input_features)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BreastCancerNet(\n",
            "  (fc1): Linear(in_features=30, out_features=64, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81f115b0"
      },
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Train the defined model using the training data. This involves defining a loss function, an optimizer, and iterating over the training data for a number of epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a8d8abe"
      },
      "source": [
        "**Reasoning**:\n",
        "Convert the training data to PyTorch tensors, create a DataLoader, define the loss function and optimizer, and then train the model for a specified number of epochs, printing the loss periodically.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3df6eb3d",
        "outputId": "b669d1e5-43f9-4d67-c269-9f7e2bbe4639"
      },
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1) # Ensure correct shape\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define Loss Function and Optimizer\n",
        "criterion = nn.BCELoss() # Binary Cross-Entropy Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop\n",
        "epochs = 100\n",
        "model.train() # Set model to training mode\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.0031\n",
            "Epoch [20/100], Loss: 0.0046\n",
            "Epoch [30/100], Loss: 0.1519\n",
            "Epoch [40/100], Loss: 0.0003\n",
            "Epoch [50/100], Loss: 0.0072\n",
            "Epoch [60/100], Loss: 0.0007\n",
            "Epoch [70/100], Loss: 0.0233\n",
            "Epoch [80/100], Loss: 0.0000\n",
            "Epoch [90/100], Loss: 0.0003\n",
            "Epoch [100/100], Loss: 0.0003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce291fa3"
      },
      "source": [
        "## Model evaluation\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model on the testing data and calculate the accuracy of the predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20fb36c2"
      },
      "source": [
        "**Reasoning**:\n",
        "Convert test data to tensors, set the model to evaluation mode, and evaluate the model on the test data to calculate accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c61827df",
        "outputId": "3c93a0f2-a0d7-45f6-9263-f27926027c57"
      },
      "source": [
        "# Convert test data to PyTorch tensors\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    # Get predictions\n",
        "    outputs = model(X_test_tensor)\n",
        "    # Apply threshold to get binary predictions\n",
        "\n",
        "    predicted = (outputs > 0.5).float()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    correct = (predicted == y_test_tensor).sum().item()\n",
        "    accuracy = correct / y_test_tensor.size(0)\n",
        "\n",
        "print(f'Accuracy of the model on the test data: {accuracy:.4f}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the test data: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5849fd88"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The 'Unnamed: 32' column was removed from the dataset as it contained only missing values.\n",
        "*   The 'diagnosis' column was successfully encoded into numerical values, with 'M' mapped to 1 and 'B' mapped to 0.\n",
        "*   The features were scaled using StandardScaler, and the data was split into training and testing sets with a test size of 20%.\n",
        "*   A deep learning model with two hidden layers and a sigmoid output layer was defined using PyTorch.\n",
        "*   The model was trained for 100 epochs using the Binary Cross-Entropy Loss function and the Adam optimizer.\n",
        "*   The accuracy of the trained model on the test data is 0.9649.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The high accuracy on the test set suggests the model is performing well in predicting breast cancer based on the provided features.\n",
        "*   Further steps could involve exploring different model architectures, hyperparameter tuning, or using cross-validation to ensure the robustness of the model's performance.\n"
      ]
    }
  ]
}